#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Web Scraper for News Articles
-----------------------------
ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆì…ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import time
from typing import Optional, Dict, List, Tuple
import re
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from enum import Enum
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# Google News URL ë””ì½”ë” (ì„ íƒì )
try:
    from googlenewsdecoder import gnewsdecoder
    GOOGLE_NEWS_DECODER_AVAILABLE = True
except ImportError:
    GOOGLE_NEWS_DECODER_AVAILABLE = False
    print("googlenewsdecoderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install googlenewsdecoderë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# newspaper3k ì„í¬íŠ¸ (ì„ íƒì )
try:
    from newspaper import Article
    NEWSPAPER3K_AVAILABLE = True
except ImportError:
    NEWSPAPER3K_AVAILABLE = False
    print("newspaper3kê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install newspaper3kë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# OpenAI API ì„í¬íŠ¸ (ì„ íƒì )
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("OpenAI APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openaië¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")


class ExtractionMethod(Enum):
    """ì¶”ì¶œ ë°©ë²• ì—´ê±°í˜•"""
    NEWSPAPER3K = "newspaper3k"
    CUSTOM_PARSER = "custom_parser"
    SELENIUM = "selenium"
    AI_FALLBACK = "ai_fallback"
    FAILED = "failed"


class ExtractionResult:
    """ì¶”ì¶œ ê²°ê³¼ í´ë˜ìŠ¤"""
    def __init__(self, title: str = "", content: str = "", url: str = "", domain: str = "", 
                 method: ExtractionMethod = ExtractionMethod.FAILED, success: bool = False, 
                 error_message: str = "", extraction_time: float = 0.0):
        self.title = title
        self.content = content
        self.url = url
        self.domain = domain
        self.method = method
        self.success = success
        self.error_message = error_message
        self.extraction_time = extraction_time
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'title': self.title,
            'content': self.content,
            'url': self.url,
            'domain': self.domain,
            'method': self.method.value,
            'success': self.success,
            'error_message': self.error_message,
            'extraction_time': self.extraction_time
        }


class HybridNewsWebScraper:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ê¸°ì‚¬ ì›¹ ìŠ¤í¬ë˜í•‘ í´ë˜ìŠ¤
    1ì°¨: newspaper3k ë²”ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
    2ì°¨: ì»¤ìŠ¤í…€ íŒŒì„œ (ì–¸ë¡ ì‚¬ë³„ íŠ¹í™”)
    3ì°¨: ê³ ê¸‰ Selenium (JavaScript ë Œë”ë§ ì‚¬ì´íŠ¸)
    4ì°¨: AI API í™œìš©
    """
    
    def __init__(self, openai_api_key: Optional[str] = None, enable_ai_fallback: bool = True):
        """ì›¹ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # AI ì„¤ì •
        self.enable_ai_fallback = enable_ai_fallback and OPENAI_AVAILABLE
        self.openai_client = None
        
        # Selenium ì„¤ì • (í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ìƒì„±)
        print("ğŸ”§ Seleniumì€ í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
        
        if OPENAI_AVAILABLE:
            try:
                # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„)
                import os
                api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
                
                if api_key:
                    # ìµœì‹  OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°©ì‹
                    try:
                        from openai import OpenAI
                        self.openai_client = OpenAI(
                            api_key=api_key,
                            base_url=os.getenv('OPENAI_BASE_URL')  # PwC ì„¤ì • ì§€ì›
                        )
                        print("âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
                    except Exception as init_error:
                        print(f"âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
                        self.enable_ai_fallback = False
                else:
                    self.enable_ai_fallback = False
                    print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ AI í´ë°±ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            except Exception as e:
                self.enable_ai_fallback = False
                print(f"âš ï¸ OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # í†µê³„ ì¶”ì 
        self.stats = {
            ExtractionMethod.NEWSPAPER3K.value: {'success': 0, 'total': 0},
            ExtractionMethod.CUSTOM_PARSER.value: {'success': 0, 'total': 0},
            ExtractionMethod.AI_FALLBACK.value: {'success': 0, 'total': 0},
            ExtractionMethod.SELENIUM.value: {'success': 0, 'total': 0},
            'total_extractions': 0,
            'total_successes': 0
        }
        
        # ì–¸ë¡ ì‚¬ë³„ ì»¤ìŠ¤í…€ íŒŒì„œ ì„¤ì • (ê°œì„ ëœ ë²„ì „)
        self.content_selectors = {
            # í•œêµ­ ì–¸ë¡ ì‚¬ (ìš°ì„ ìˆœìœ„ë³„ ì„ íƒì)
            'chosun.com': {
                'content': ['div.article-body', 'div.news_text', '.article_view', 'div.par'],
                'title': ['h1.headline', 'h1', '.article-title', 'title']
            },
            'biz.chosun.com': {
                'content': ['div.article-body', 'div.news_text', '.article_view', 'div.par', 'div.article_txt', '.story_content'],
                'title': ['h1.headline', 'h1', '.article-title', '.news_title', 'title']
            },
            'joongang.co.kr': {
                'content': ['div.article_body', 'div.article-body', '.article_content', 'div.article_content'],
                'title': ['h1.headline', 'h1', '.article-title']
            },
            'donga.com': {
                'content': ['div.article_txt', 'div.article-body', 'div.article_content'],
                'title': ['h1.title', 'h1', '.article-title']
            },
            'mk.co.kr': {
                'content': ['div.art_txt', 'div.news_cnt_detail_wrap', 'div.news_content'],
                'title': ['h1.news_ttl', 'h1', '.news-title']
            },
            'hankyung.com': {
                'content': ['div.article-body', 'div.news-content', 'div.article_txt'],
                'title': ['h1.headline', 'h1', '.article-title']
            },
            'fnnews.com': {
                'content': ['div.article-body', 'div.article_content', 'div.news_content'],
                'title': ['h1.article-headline', 'h1', '.article-title']
            },
            'mt.co.kr': {
                'content': ['div.article-body', 'div.news_text', 'div.news_content'],
                'title': ['h1.subject', 'h1', '.news-title']
            },
            'edaily.co.kr': {
                'content': ['div.news_body', 'div.article-body', 'div.news_content'],
                'title': ['h1.news_title', 'h1', '.article-title']
            },
            'asiae.co.kr': {
                'content': ['div.article-body', 'div.view_con', 'div.article_content'],
                'title': ['h1.article_title', 'h1', '.article-title']
            },
            'newsis.com': {
                'content': ['div.article-body', 'div.article_content', 'div.news_content'],
                'title': ['h1.article_title', 'h1', '.article-title']
            },
            'yonhapnews.co.kr': {
                'content': ['div.article-body', 'div.article_text', 'div.story-body'],
                'title': ['h1.tit', 'h1', '.article-title']
            },
            
            # í•´ì™¸ ì–¸ë¡ ì‚¬
            'reuters.com': {
                'content': ['div.ArticleBodyWrapper', 'div.StandardArticleBody_body', 'div.ArticleBody'],
                'title': ['h1[data-testid="Headline"]', 'h1', '.ArticleHeader_headline']
            },
            'bloomberg.com': {
                'content': ['div.body-content', 'div.article-body', 'div[data-module="ArticleBody"]'],
                'title': ['h1[data-module="ArticleHeader"]', 'h1', '.headline']
            },
            'yahoo.com': {
                'content': ['div.caas-body', 'div.article-body', 'div[data-module="ArticleBody"]'],
                'title': ['h1[data-test-locator="headline"]', 'h1', '.headline']
            },
            'automotive-news.com': {
                'content': ['div.article-body', 'div.story-body', 'div.field-name-body'],
                'title': ['h1.headline', 'h1', '.article-title']
            },
            'motor1.com': {
                'content': ['div.article-body', 'div.content-body', 'div.post-content'],
                'title': ['h1.title', 'h1', '.article-title']
            },
            
            # ê¸°ë³¸ ì„ íƒì (ìš°ì„ ìˆœìœ„ë³„)
            'default': {
                'content': ['article', 'div.article-body', 'div.article_body', 'div.content', 'div.post-content', '.article-content', '.news-content', '.story-body', 'main', '[role="main"]'],
                'title': ['h1', '.article-title', '.news-title', '.headline', '.title', 'title']
            }
        }
    
    def extract_content(self, url: str, timeout: int = 15) -> ExtractionResult:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        1ì°¨: newspaper3k -> 2ì°¨: ì»¤ìŠ¤í…€ íŒŒì„œ -> 3ì°¨: ê³ ê¸‰ Selenium -> 4ì°¨: AI API
        
        Args:
            url (str): ê¸°ì‚¬ URL (Google News RSS URL í¬í•¨)
            timeout (int): ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ê°’: 15ì´ˆ)
            
        Returns:
            ExtractionResult: ì¶”ì¶œ ê²°ê³¼ ê°ì²´
        """
        start_time = time.time()
        self.stats['total_extractions'] += 1
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not url or not url.startswith(('http://', 'https://')):
            return ExtractionResult(
                url=url, 
                error_message="ìœ íš¨í•˜ì§€ ì•Šì€ URL",
                extraction_time=time.time() - start_time
            )
        
        # Google News RSS URL ì²˜ë¦¬
        original_url = url
        if 'news.google.com/rss/articles/' in url or 'news.google.com/read/' in url:
            print(f"   ğŸ”— Google News URL ê°ì§€ - ì›ë³¸ URL ì¶”ì¶œ ì¤‘...")
            actual_url = self._resolve_google_news_url_simple(url, timeout)
            if actual_url and actual_url != url:
                url = actual_url
                print(f"   âœ… ì›ë³¸ URL ì¶”ì¶œ ì„±ê³µ: {url}")
            else:
                print(f"   âš ï¸ ì›ë³¸ URL ì¶”ì¶œ ì‹¤íŒ¨ - ì›ë³¸ URLë¡œ ê³„ì† ì§„í–‰")
        
        domain = urlparse(url).netloc.lower()
        
        # 1ì°¨: newspaper3k ì‹œë„
        if NEWSPAPER3K_AVAILABLE:
            print(f"   ğŸ“° 1ì°¨ newspaper3k ì‹œë„ ì¤‘...")
            result = self._extract_with_newspaper3k(url, domain, timeout)
            if result.success:
                result.extraction_time = time.time() - start_time
                result.url = original_url  # ì›ë³¸ URLë¡œ ì„¤ì •
                self._update_stats(ExtractionMethod.NEWSPAPER3K, True)
                self.stats['total_successes'] += 1
                print(f"   âœ… newspaper3k ì„±ê³µ!")
                return result
            else:
                print(f"   âŒ newspaper3k ì‹¤íŒ¨: {result.error_message}")
            self._update_stats(ExtractionMethod.NEWSPAPER3K, False)
        else:
            print(f"   âš ï¸ newspaper3k ì‚¬ìš© ë¶ˆê°€")
        
        # 2ì°¨: ì»¤ìŠ¤í…€ íŒŒì„œ ì‹œë„
        print(f"   ğŸ”§ 2ì°¨ ì»¤ìŠ¤í…€ íŒŒì„œ ì‹œë„ ì¤‘...")
        result = self._extract_with_custom_parser(url, domain, timeout)
        if result.success:
            result.extraction_time = time.time() - start_time
            result.url = original_url  # ì›ë³¸ URLë¡œ ì„¤ì •
            self._update_stats(ExtractionMethod.CUSTOM_PARSER, True)
            self.stats['total_successes'] += 1
            print(f"   âœ… ì»¤ìŠ¤í…€ íŒŒì„œ ì„±ê³µ!")
            return result
        else:
            print(f"   âŒ ì»¤ìŠ¤í…€ íŒŒì„œ ì‹¤íŒ¨: {result.error_message}")
        self._update_stats(ExtractionMethod.CUSTOM_PARSER, False)
        
        # 3ì°¨: ê³ ê¸‰ Selenium ì‹œë„ (ì¡°ì„ ë¹„ì¦ˆ ë“± JavaScript ì‚¬ì´íŠ¸)
        if domain in ['biz.chosun.com']:
            print(f"   ğŸ¢ 3ì°¨ ê³ ê¸‰ Selenium ì‹œë„ ì¤‘...")
            result = self._extract_with_improved_selenium(url, domain, timeout)
            if result.success:
                result.extraction_time = time.time() - start_time
                result.url = original_url  # ì›ë³¸ URLë¡œ ì„¤ì •
                self._update_stats(ExtractionMethod.SELENIUM, True)
                self.stats['total_successes'] += 1
                print(f"   âœ… ê³ ê¸‰ Selenium ì„±ê³µ!")
                return result
            else:
                print(f"   âŒ ê³ ê¸‰ Selenium ì‹¤íŒ¨: {result.error_message}")
            self._update_stats(ExtractionMethod.SELENIUM, False)
        
        # 4ì°¨: AI API ì‹œë„ (í™œì„±í™”ëœ ê²½ìš°)
        if self.enable_ai_fallback:
            print(f"   ğŸ¤– 4ì°¨ AI í´ë°± ì‹œë„ ì¤‘...")
            result = self._extract_with_ai_fallback(url, domain, timeout)
            if result.success:
                result.extraction_time = time.time() - start_time
                result.url = original_url  # ì›ë³¸ URLë¡œ ì„¤ì •
                self._update_stats(ExtractionMethod.AI_FALLBACK, True)
                self.stats['total_successes'] += 1
                print(f"   âœ… AI í´ë°± ì„±ê³µ!")
                return result
            else:
                print(f"   âŒ AI í´ë°± ì‹¤íŒ¨: {result.error_message}")
            self._update_stats(ExtractionMethod.AI_FALLBACK, False)
        else:
            print(f"   âš ï¸ AI í´ë°±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        return ExtractionResult(
            url=original_url,  # ì›ë³¸ Google News URL ìœ ì§€
            domain=domain,
            error_message="ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨",
            extraction_time=time.time() - start_time
        )
    
    def _extract_with_newspaper3k(self, url: str, domain: str, timeout: int) -> ExtractionResult:
        """1ì°¨: newspaper3k ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì¶”ì¶œ"""
        try:
            article = Article(url)
            article.download()
            article.parse()
            
            # ìµœì†Œ í’ˆì§ˆ ê²€ì¦
            if len(article.text) < 100 or not article.title:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.NEWSPAPER3K,
                    error_message="newspaper3k: ì¶”ì¶œëœ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ"
                )
            
            cleaned_content = self._clean_content(article.text)
            
            return ExtractionResult(
                title=article.title.strip(),
                content=cleaned_content,
                url=url,
                domain=domain,
                method=ExtractionMethod.NEWSPAPER3K,
                success=True
            )
            
        except Exception as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.NEWSPAPER3K,
                error_message=f"newspaper3k ì˜¤ë¥˜: {str(e)}"
            )
    
    def _extract_with_selenium(self, url: str, domain: str, timeout: int) -> ExtractionResult:
        """3ì°¨: Seleniumìœ¼ë¡œ ë™ì  ì½˜í…ì¸  ì¶”ì¶œ"""
        if not self.driver:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                error_message="Selenium ë“œë¼ì´ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
            )
        
        try:
            print("   ğŸŒ Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            self.driver.get(url)
            
            # JavaScript ë Œë”ë§ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, timeout)
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(2)  # ì¶”ê°€ ëŒ€ê¸°
            
            # ë Œë”ë§ëœ HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
            title = ""
            content = ""
            
            # ë„ë©”ì¸ë³„ ì„ íƒìë¡œ ì‹œë„
            selectors = self.content_selectors.get(domain, self.content_selectors['default'])
            
            # ì œëª© ì¶”ì¶œ
            for title_selector in selectors['title']:
                title_elem = soup.select_one(title_selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            for content_selector in selectors['content']:
                content_elem = soup.select_one(content_selector)
                if content_elem:
                    content = content_elem.get_text().strip()
                    break
            
            # ê²°ê³¼ ê²€ì¦
            if not content or len(content.strip()) < 100:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                    error_message="Selenium: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±"
                )
            
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                title=title, content=self._clean_content(content)
            )
            
        except TimeoutException:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                error_message="Selenium: í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼"
            )
        except WebDriverException as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                error_message=f"Selenium ì˜¤ë¥˜: {str(e)}"
            )
        except Exception as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                error_message=f"Selenium ì˜ˆì™¸ ë°œìƒ: {str(e)}"
            )
    
    def _extract_with_improved_selenium(self, url: str, domain: str, timeout: int) -> ExtractionResult:
        """3ì°¨: ImprovedNewsExtractorì˜ ê³ ê¸‰ Selenium ì¶”ì¶œ"""
        print("   ğŸŒ ê³ ê¸‰ Selenium ëª¨ë“œë¡œ ì‹œë„...")
        
        # ê³ ê¸‰ Chrome ì˜µì…˜ ì„¤ì • (ImprovedNewsExtractor ë°©ì‹)
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-web-security')
        options.add_argument('--disable-features=VizDisplayCompositor')
        
        # Google ê´€ë ¨ ì—ëŸ¬ ë°©ì§€ (ê³ ê¸‰ ì˜µì…˜)
        options.add_argument('--disable-background-networking')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-breakpad')
        options.add_argument('--disable-client-side-phishing-detection')
        options.add_argument('--disable-component-extensions-with-background-pages')
        options.add_argument('--disable-default-apps')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-hang-monitor')
        options.add_argument('--disable-ipc-flooding-protection')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--disable-prompt-on-repost')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-sync')
        options.add_argument('--force-fieldtrials=*BackgroundTracing/default/')
        options.add_argument('--metrics-recording-only')
        options.add_argument('--no-crash-upload')
        options.add_argument('--no-first-run')
        
        options.add_experimental_option('excludeSwitches', ['enable-automation', 'enable-logging'])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option("prefs", {
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_settings.popups": 0,
            "profile.managed_default_content_settings.images": 2
        })
        
        # ì¡°ì„ ë¹„ì¦ˆ ì „ìš© ë‹¤ì¤‘ ì…€ë ‰í„°
        chosun_selectors = {
            'title': [
                'h1.article-header__headline',
                'h1.news_title_text',
                '.article-title h1',
                'h1[class*="title"]',
                '.headline h1',
                '.news-headline',
                'h1'
            ],
            'content': [
                'div.article-body',
                'div.news_text_area', 
                '.article-content',
                '#article_body',
                '.article-text',
                '.news-content',
                '.content-body',
                'div[class*="content"]',
                'div[class*="article"]'
            ]
        }
        
        driver = None
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            
            # ë´‡ íƒì§€ ìš°íšŒ
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("   ğŸŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            driver.get(url)
            
            # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, timeout).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸ )
            time.sleep(3)
            
            # ìŠ¤í¬ë¡¤í•´ì„œ ëª¨ë“  ì½˜í…ì¸  ë¡œë”©
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (ë‹¤ì¤‘ ì…€ë ‰í„°)
            title = ""
            for selector in chosun_selectors['title']:
                try:
                    element = soup.select_one(selector)
                    if element:
                        title = element.get_text().strip()
                        if title and len(title) > 10:
                            break
                except Exception:
                    continue
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ê³ ê¸‰ ë°©ì‹)
            content = ""
            for selector in chosun_selectors['content']:
                try:
                    element = soup.select_one(selector)
                    if element:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in element.find_all(['script', 'style', 'nav', 'header', 'footer', 
                                                         '.ad', '.advertisement', '.related', '.comment',
                                                         '[class*="ad"]', '[id*="ad"]']):
                            unwanted.decompose()
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
                        text = element.get_text(separator='\n', strip=True)
                        
                        # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ í•„í„°ë§
                        lines = [line.strip() for line in text.split('\n') if line.strip()]
                        meaningful_lines = [line for line in lines if len(line) > 20 and not self._is_junk_line(line)]
                        
                        if len(meaningful_lines) >= 3:
                            content = '\n\n'.join(meaningful_lines)
                            break
                            
                except Exception:
                    continue
            
            # í´ë°±: ì „ì²´ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì •
            if not content:
                content = self._fallback_content_extraction(soup)
            
            if title and content and len(content) > 200:
                print("   âœ… ê³ ê¸‰ Seleniumìœ¼ë¡œ ì„±ê³µì  ì¶”ì¶œ!")
                return ExtractionResult(
                    title=title,
                    content=self._clean_content(content),
                    url=url,
                    domain=domain,
                    method=ExtractionMethod.SELENIUM,
                    success=True
                )
            else:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                    error_message="ê³ ê¸‰ Selenium: ì¶©ë¶„í•œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
                )
                
        except Exception as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.SELENIUM,
                error_message=f"ê³ ê¸‰ Selenium ì˜¤ë¥˜: {str(e)}"
            )
        finally:
            if driver:
                driver.quit()
    
    def _extract_with_custom_parser(self, url: str, domain: str, timeout: int) -> ExtractionResult:
        """2ì°¨: ì»¤ìŠ¤í…€ íŒŒì„œë¥¼ ì‚¬ìš©í•œ ì¶”ì¶œ"""
        try:
            # HTTP ìš”ì²­
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ì„¤ì •
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title_custom(soup, domain)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_custom(soup, domain)
            
            if not content or len(content) < 100:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.CUSTOM_PARSER,
                    error_message="ì»¤ìŠ¤í…€ íŒŒì„œ: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±"
                )
            
            # ë³¸ë¬¸ ì •ë¦¬
            cleaned_content = self._clean_content(content)
            
            return ExtractionResult(
                title=title,
                content=cleaned_content,
                url=url,
                domain=domain,
                method=ExtractionMethod.CUSTOM_PARSER,
                success=True
            )
            
        except requests.exceptions.Timeout:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.CUSTOM_PARSER,
                error_message="ì»¤ìŠ¤í…€ íŒŒì„œ: ìš”ì²­ íƒ€ì„ì•„ì›ƒ"
            )
        except Exception as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.CUSTOM_PARSER,
                error_message=f"ì»¤ìŠ¤í…€ íŒŒì„œ ì˜¤ë¥˜: {str(e)}"
            )
    
    def _extract_with_ai_fallback(self, url: str, domain: str, timeout: int) -> ExtractionResult:
        """3ì°¨: AI APIë¥¼ ì‚¬ìš©í•œ ì¶”ì¶œ"""
        try:
            # ë¨¼ì € HTML ê°€ì ¸ì˜¤ê¸°
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
            
            # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ì „ì²˜ë¦¬)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
                tag.decompose()
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë” ê´€ëŒ€í•œ ê¸¸ì´ ì œí•œ)
            raw_text = soup.get_text()[:8000]  # AI API í† í° ì œí•œ ê³ ë ¤í•˜ë˜ ë” ë§ì´
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬ë¥¼ ë” ê´€ëŒ€í•˜ê²Œ (50ì ì´ìƒì´ë©´ ì‹œë„)
            if len(raw_text.strip()) < 50:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.AI_FALLBACK,
                    error_message=f"AI í´ë°±: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(raw_text.strip())}ì)"
                )
            
            print(f"   ğŸ“„ AI ë¶„ì„ìš© í…ìŠ¤íŠ¸: {len(raw_text.strip())}ì")
            
            # OpenAI API í˜¸ì¶œ
            prompt = f"""
            ë‹¤ìŒì€ ë‰´ìŠ¤ ì›¹í˜ì´ì§€ì˜ HTMLì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
            ì´ ì¤‘ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
            ê´‘ê³ , ë©”ë‰´, ëŒ“ê¸€ ë“±ì€ ì œì™¸í•˜ê³  ìˆœìˆ˜í•œ ê¸°ì‚¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
            
            ì‘ë‹µ í˜•ì‹:
            {{
                "title": "ê¸°ì‚¬ ì œëª©",
                "content": "ê¸°ì‚¬ ë³¸ë¬¸"
            }}
            
            ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸:
            {raw_text}
            """
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ í™•ì¸
            if not self.openai_client:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.AI_FALLBACK,
                    error_message="OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
                )
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì›¹í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            # AI ì‘ë‹µ íŒŒì‹±
            ai_response = response.choices[0].message.content.strip()
            
            try:
                parsed_result = json.loads(ai_response)
                title = parsed_result.get('title', 'ì œëª© ì—†ìŒ')
                content = parsed_result.get('content', '')
                
                if len(content) < 100:
                    return ExtractionResult(
                        url=url, domain=domain, method=ExtractionMethod.AI_FALLBACK,
                        error_message="AI ì¶”ì¶œ: ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ"
                    )
                
                cleaned_content = self._clean_content(content)
                
                return ExtractionResult(
                    title=title,
                    content=cleaned_content,
                    url=url,
                    domain=domain,
                    method=ExtractionMethod.AI_FALLBACK,
                    success=True
                )
                
            except json.JSONDecodeError:
                return ExtractionResult(
                    url=url, domain=domain, method=ExtractionMethod.AI_FALLBACK,
                    error_message="AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                )
                
        except Exception as e:
            return ExtractionResult(
                url=url, domain=domain, method=ExtractionMethod.AI_FALLBACK,
                error_message=f"AI API ì˜¤ë¥˜: {str(e)}"
            )
    
    def _extract_title_custom(self, soup: BeautifulSoup, domain: str) -> str:
        """ì»¤ìŠ¤í…€ ì œëª© ì¶”ì¶œ"""
        # ë„ë©”ì¸ë³„ ì œëª© ì„ íƒì ì‹œë„
        domain_config = self.content_selectors.get(domain, self.content_selectors['default'])
        title_selectors = domain_config.get('title', self.content_selectors['default']['title'])
        
        for selector in title_selectors:
            try:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title and len(title) > 5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        return title
            except Exception:
                continue
        
        return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
    
    def _extract_content_custom(self, soup: BeautifulSoup, domain: str) -> Optional[str]:
        """ì»¤ìŠ¤í…€ ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë„ë©”ì¸ë³„ ë³¸ë¬¸ ì„ íƒì ì‹œë„
        domain_config = self.content_selectors.get(domain, self.content_selectors['default'])
        content_selectors = domain_config.get('content', self.content_selectors['default']['content'])
        
        for selector in content_selectors:
            try:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text().strip()
                    if content and len(content) > 100:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        return content
            except Exception:
                continue
        
        return None
    
    def _clean_content(self, content: str) -> str:
        """ë³¸ë¬¸ ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        if not content:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        content = re.sub(r'\s+', ' ', content)
        
        # ê´‘ê³ ì„± ë¬¸êµ¬ ì œê±° (í™•ì¥ëœ íŒ¨í„´)
        ad_patterns = [
            r'ì €ì‘ê¶Œì.*?ë¬´ë‹¨.*?ê¸ˆì§€',
            r'Copyright.*?All rights reserved',
            r'ë¬´ë‹¨ì „ì¬.*?ì¬ë°°í¬.*?ê¸ˆì§€',
            r'ê¸°ì.*?@.*?\.com',
            r'â–¶.*?ë°”ë¡œê°€ê¸°',
            r'â–².*?ì‚¬ì§„',
            r'\[.*?ê¸°ì\]',
            r'ì´ ê¸°ì‚¬ëŠ”.*?ì œê³µ',
            r'ê´€ë ¨ê¸°ì‚¬.*?ë”ë³´ê¸°',
            r'êµ¬ë….*?ì•Œë¦¼',
            r'ëŒ“ê¸€.*?ë‚¨ê¸°ê¸°',
            r'ê³µìœ í•˜ê¸°',
            r'í”„ë¦°íŠ¸.*?ìŠ¤í¬ë©',
            r'ì¢‹ì•„ìš”.*?ì‹«ì–´ìš”',
            r'SNS.*?ê³µìœ ',
            r'ë„¤ì´ë²„.*?ë‹¤ìŒ',
            r'í˜ì´ìŠ¤ë¶.*?íŠ¸ìœ„í„°',
            r'ì¹´ì¹´ì˜¤.*?ë¼ì¸',
            r'ê´‘ê³ .*?AD',
            r'Sponsored.*?Content'
        ]
        
        for pattern in ad_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        content = re.sub(r'\s+', ' ', content).strip()
        content = re.sub(r'\n+', '\n', content)
        
        # ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ê¸°ì‚¬ëŠ” ì•ë¶€ë¶„ë§Œ)
        if len(content) > 3000:
            content = content[:3000] + "..."
        
        return content
    
    def _is_junk_line(self, line):
        """ì“¸ëª¨ì—†ëŠ” ë¼ì¸ íŒë³„ (ImprovedNewsExtractorì—ì„œ ê°€ì ¸ì˜´)"""
        junk_patterns = [
            r'^\s*\d+\s*$',  # ìˆ«ìë§Œ
            r'^\s*[^\w\s]*\s*$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'ê´€ë ¨\s*ê¸°ì‚¬',
            r'ë”\s*ë³´ê¸°',
            r'ì´ì „\s*ê¸°ì‚¬',
            r'ë‹¤ìŒ\s*ê¸°ì‚¬',
            r'^\s*AD\s*$',
            r'ê´‘ê³ ',
            r'êµ¬ë…',
            r'ë¡œê·¸ì¸',
            r'íšŒì›ê°€ì…'
        ]
        
        for pattern in junk_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                return True
        return False
    
    def _fallback_content_extraction(self, soup):
        """í´ë°± ë³¸ë¬¸ ì¶”ì¶œ (ImprovedNewsExtractorì—ì„œ ê°€ì ¸ì˜´)"""
        # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        all_divs = soup.find_all(['div', 'article', 'section', 'main'])
        best_content = ""
        
        for div in all_divs:
            text = div.get_text().strip()
            if len(text) > len(best_content) and len(text) > 300:
                # ë„ˆë¬´ ë§ì€ ë§í¬ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
                links = div.find_all('a')
                if len(links) < len(text) / 100:  # í…ìŠ¤íŠ¸ 100ìë‹¹ ë§í¬ 1ê°œ ë¯¸ë§Œ
                    best_content = text
        
        return best_content
    
    def _resolve_google_news_url_simple(self, google_news_url: str, timeout: int = 10) -> Optional[str]:
        """Google News URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URLì„ ì¶”ì¶œ (googlenewsdecoder ì‚¬ìš©)"""
        try:
            # 1ì°¨: googlenewsdecoder ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            if GOOGLE_NEWS_DECODER_AVAILABLE:
                print(f"   ğŸ“¦ googlenewsdecoder ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì¤‘...")
                try:
                    result = gnewsdecoder(google_news_url, interval=1)
                    if result.get("status") and result.get("decoded_url"):
                        decoded_url = result["decoded_url"]
                        print(f"   âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë””ì½”ë”© ì„±ê³µ: {decoded_url}")
                        return decoded_url
                    else:
                        print(f"   âš ï¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë””ì½”ë”© ì‹¤íŒ¨: {result.get('message', 'Unknown error')}")
                except Exception as e:
                    print(f"   âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜: {str(e)}")
            
            # 2ì°¨: í´ë°± - ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ê°€ê¸°
            print(f"   ğŸŒ ë¦¬ë‹¤ì´ë ‰íŠ¸ í´ë°± ì‹œë„ ì¤‘...")
            response = self.session.get(
                google_news_url, 
                timeout=timeout,
                allow_redirects=True,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
            )
            
            final_url = response.url
            if final_url and 'google.com' not in final_url and final_url != google_news_url:
                print(f"   âœ… ë¦¬ë‹¤ì´ë ‰íŠ¸ë¡œ ì›ë³¸ URL ë°œê²¬: {final_url}")
                return final_url
            
            print(f"   âŒ ì›ë³¸ URL ì¶”ì¶œ ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            print(f"   âŒ Google News URL í•´ì„ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _update_stats(self, method: ExtractionMethod, success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        method_key = method.value
        self.stats[method_key]['total'] += 1
        if success:
            self.stats[method_key]['success'] += 1
    
    def get_stats(self) -> Dict:
        """ì¶”ì¶œ í†µê³„ ë°˜í™˜"""
        stats_copy = self.stats.copy()
        
        # ì„±ê³µë¥  ê³„ì‚°
        for method in [ExtractionMethod.NEWSPAPER3K, ExtractionMethod.CUSTOM_PARSER, ExtractionMethod.AI_FALLBACK]:
            method_key = method.value
            total = stats_copy[method_key]['total']
            success = stats_copy[method_key]['success']
            stats_copy[method_key]['success_rate'] = (success / total * 100) if total > 0 else 0
        
        # ì „ì²´ ì„±ê³µë¥ 
        total_extractions = stats_copy['total_extractions']
        total_successes = stats_copy['total_successes']
        stats_copy['overall_success_rate'] = (total_successes / total_extractions * 100) if total_extractions > 0 else 0
        
        return stats_copy
    
    def extract_multiple_articles(self, urls: List[str], parallel: bool = True, max_workers: int = 5, delay: float = 0.5) -> Dict[str, ExtractionResult]:
        """
        ì—¬ëŸ¬ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ ì¼ê´„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›).
        
        Args:
            urls (List[str]): URL ë¦¬ìŠ¤íŠ¸
            parallel (bool): ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            max_workers (int): ìµœëŒ€ ë™ì‹œ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            delay (float): ìˆœì°¨ ì²˜ë¦¬ ì‹œ ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ê¸°ë³¸ê°’: 0.5ì´ˆ)
            
        Returns:
            Dict[str, ExtractionResult]: URLì„ í‚¤ë¡œ í•˜ê³  ì¶”ì¶œ ê²°ê³¼ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        if parallel and len(urls) > 1:
            # ë³‘ë ¬ ì²˜ë¦¬
            print(f"ë³‘ë ¬ ì²˜ë¦¬ë¡œ {len(urls)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì‹œì‘ (ìµœëŒ€ {max_workers}ê°œ ìŠ¤ë ˆë“œ)")
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  URLì— ëŒ€í•´ ì¶”ì¶œ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                future_to_url = {
                    executor.submit(self.extract_content, url): url 
                    for url in urls
                }
                
                # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        result = future.result()
                        results[url] = result
                        status = "âœ…" if result.success else "âŒ"
                        method = result.method.value if result.success else "ì‹¤íŒ¨"
                        print(f"{status} {url} - {method}")
                    except Exception as e:
                        results[url] = ExtractionResult(
                            url=url, 
                            error_message=f"ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                        )
                        print(f"âŒ {url} - ì˜¤ë¥˜: {str(e)}")
            
            elapsed_time = time.time() - start_time
            success_count = sum(1 for result in results.values() if result.success)
            print(f"ë³‘ë ¬ ì¶”ì¶œ ì™„ë£Œ: {success_count}/{len(urls)}ê°œ ì„±ê³µ, ì†Œìš”ì‹œê°„ {elapsed_time:.2f}ì´ˆ")
            
        else:
            # ìˆœì°¨ ì²˜ë¦¬
            print(f"ìˆœì°¨ ì²˜ë¦¬ë¡œ {len(urls)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì‹œì‘")
            start_time = time.time()
        
        for i, url in enumerate(urls, 1):
            print(f"ê¸°ì‚¬ {i}/{len(urls)} ì¶”ì¶œ ì¤‘: {url}")
            
            result = self.extract_content(url)
            results[url] = result
            
            status = "âœ…" if result.success else "âŒ"
            method = result.method.value if result.success else "ì‹¤íŒ¨"
            print(f"{status} ì™„ë£Œ - {method}")
            
            # ìš”ì²­ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(urls):
                time.sleep(delay)
            
            elapsed_time = time.time() - start_time
            success_count = sum(1 for result in results.values() if result.success)
            print(f"ìˆœì°¨ ì¶”ì¶œ ì™„ë£Œ: {success_count}/{len(urls)}ê°œ ì„±ê³µ, ì†Œìš”ì‹œê°„ {elapsed_time:.2f}ì´ˆ")
        
        return results


# í•˜ì´ë¸Œë¦¬ë“œ ì›¹ìŠ¤í¬ë˜í¼ ë³„ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)
NewsWebScraper = HybridNewsWebScraper


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = HybridNewsWebScraper(
        # openai_api_key="your-api-key-here",  # AI í´ë°± ì‚¬ìš© ì‹œ
        enable_ai_fallback=False  # AI í´ë°± ë¹„í™œì„±í™” (API í‚¤ ì—†ì„ ë•Œ)
    )
    
    # í…ŒìŠ¤íŠ¸ URL (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” URLë¡œ í…ŒìŠ¤íŠ¸)
    test_urls = [
        "https://www.hankyung.com/economy/article/2024010112345",  # í•œêµ­ê²½ì œ ì˜ˆì‹œ
        "https://www.mk.co.kr/news/economy/10891234",  # ë§¤ì¼ê²½ì œ ì˜ˆì‹œ
        "https://www.reuters.com/business/autos-transportation/hyundai-motor-2024-01-01/",  # Reuters ì˜ˆì‹œ
        "https://news.yahoo.com/hyundai-electric-vehicle-2024-123456.html"  # Yahoo ì˜ˆì‹œ
    ]
    
    print("=== í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ===")
    print(f"newspaper3k ì‚¬ìš© ê°€ëŠ¥: {NEWSPAPER3K_AVAILABLE}")
    print(f"OpenAI API ì‚¬ìš© ê°€ëŠ¥: {OPENAI_AVAILABLE}")
    print(f"AI í´ë°± í™œì„±í™”: {scraper.enable_ai_fallback}")
    print()
    
    print("=== ë‹¨ì¼ ê¸°ì‚¬ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===")
    for i, url in enumerate(test_urls, 1):
        print(f"\n[{i}] ì¶”ì¶œ ì¤‘: {url}")
        result = scraper.extract_content(url)
        
        if result.success:
            print(f"âœ… ì„±ê³µ - ë°©ë²•: {result.method.value}")
            print(f"   ì œëª©: {result.title}")
            print(f"   ë³¸ë¬¸ (ì• 200ì): {result.content[:200]}...")
            print(f"   ë„ë©”ì¸: {result.domain}")
            print(f"   ì¶”ì¶œ ì‹œê°„: {result.extraction_time:.2f}ì´ˆ")
        else:
            print(f"âŒ ì‹¤íŒ¨ - {result.error_message}")
        print("-" * 80)
    
    print("\n=== ë³‘ë ¬ ì¼ê´„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===")
    results = scraper.extract_multiple_articles(test_urls, parallel=True, max_workers=3)
    
    print(f"\n=== ì¶”ì¶œ ê²°ê³¼ ìš”ì•½ ===")
    success_count = sum(1 for result in results.values() if result.success)
    print(f"ì „ì²´: {len(results)}ê°œ, ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {len(results) - success_count}ê°œ")
    
    # ë°©ë²•ë³„ í†µê³„
    method_stats = {}
    for result in results.values():
        if result.success:
            method = result.method.value
            method_stats[method] = method_stats.get(method, 0) + 1
    
    print("\në°©ë²•ë³„ ì„±ê³µ í†µê³„:")
    for method, count in method_stats.items():
        print(f"  - {method}: {count}ê°œ")
    
    print("\n=== ì „ì²´ í†µê³„ ===")
    stats = scraper.get_stats()
    for method in ['newspaper3k', 'custom_parser', 'ai_fallback']:
        total = stats[method]['total']
        success = stats[method]['success']
        rate = stats[method]['success_rate']
        print(f"{method}: {success}/{total} ({rate:.1f}%)")
    
    print(f"ì „ì²´ ì„±ê³µë¥ : {stats['overall_success_rate']:.1f}%")
