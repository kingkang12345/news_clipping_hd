import requests
import time
import re
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

class ImprovedNewsExtractor:
    """ì¡°ì„ ì¼ë³´ ë° ê¸°íƒ€ ì–¸ë¡ ì‚¬ ìµœì í™” ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.session = requests.Session()
        # ë” ê°•ë ¥í•œ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        # ì¡°ì„ ì¼ë³´ ì „ìš© ë‹¤ì¤‘ ì…€ë ‰í„°
        self.chosun_selectors = {
            'title': [
                'h1.article-header__headline',
                'h1.news_title_text',
                '.article-title h1',
                'h1[class*="title"]',
                '.headline h1',
                '.news-headline',
                'h1'  # ë§ˆì§€ë§‰ í´ë°±
            ],
            'content': [
                'div.article-body',
                'div.news_text_area', 
                '.article-content',
                '#article_body',
                '.article-text',
                '.news-content',
                '.content-body',
                'div[class*="content"]',
                'div[class*="article"]'
            ],
            'date': [
                'time.article-header__date',
                'span.news_date',
                '.date',
                'time',
                '[datetime]'
            ]
        }
    
    def extract_chosun_biz_advanced(self, url):
        """ì¡°ì„ ì¼ë³´ ë¹„ì¦ˆ ê³ ê¸‰ ì¶”ì¶œê¸°"""
        print(f"ğŸ¢ ì¡°ì„ ì¼ë³´ ë¹„ì¦ˆ ì „ìš© ì¶”ì¶œê¸° ì‹¤í–‰: {url}")
        
        try:
            # 1ë‹¨ê³„: ì¼ë°˜ ìš”ì²­
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_with_selectors(soup, self.chosun_selectors['title'])
            print(f"ğŸ“° ì¶”ì¶œëœ ì œëª©: {title[:100] if title else 'None'}...")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_advanced(soup)
            print(f"ğŸ“„ ì¶”ì¶œëœ ë³¸ë¬¸ ê¸¸ì´: {len(content) if content else 0}ì")
            
            # ë°œí–‰ì¼ ì¶”ì¶œ
            date = self._extract_with_selectors(soup, self.chosun_selectors['date'])
            
            if title and content and len(content) > 200:
                return {
                    'title': title,
                    'content': content,
                    'publish_date': date,
                    'url': url,
                    'method': 'ì¡°ì„ ì¼ë³´ ì „ìš© íŒŒì„œ',
                    'confidence': 0.9,
                    'success': True
                }
            else:
                print("âš ï¸ ì¶”ì¶œ ê²°ê³¼ ë¶€ì¡± - Selenium ì‹œë„ í•„ìš”")
                return self._extract_with_selenium_advanced(url)
                
        except Exception as e:
            print(f"âŒ ì¼ë°˜ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return self._extract_with_selenium_advanced(url)
    
    def _extract_with_selectors(self, soup, selectors):
        """ë‹¤ì¤‘ ì…€ë ‰í„°ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for selector in selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text().strip()
                    if text and len(text) > 10:
                        return text
            except Exception:
                continue
        return None
    
    def _extract_content_advanced(self, soup):
        """ê³ ê¸‰ ë³¸ë¬¸ ì¶”ì¶œ"""
        for selector in self.chosun_selectors['content']:
            try:
                element = soup.select_one(selector)
                if element:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for unwanted in element.find_all(['script', 'style', 'nav', 'header', 'footer', 
                                                     '.ad', '.advertisement', '.related', '.comment',
                                                     '[class*="ad"]', '[id*="ad"]']):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
                    text = element.get_text(separator='\n', strip=True)
                    
                    # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ í•„í„°ë§
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    meaningful_lines = [line for line in lines if len(line) > 20 and not self._is_junk_line(line)]
                    
                    if len(meaningful_lines) >= 3:
                        return '\n\n'.join(meaningful_lines)
                        
            except Exception as e:
                continue
        
        # í´ë°±: ì „ì²´ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì •
        return self._fallback_content_extraction(soup)
    
    def _is_junk_line(self, line):
        """ì“¸ëª¨ì—†ëŠ” ë¼ì¸ íŒë³„"""
        junk_patterns = [
            r'^\s*\d+\s*$',  # ìˆ«ìë§Œ
            r'^\s*[^\w\s]*\s*$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            r'ê´€ë ¨\s*ê¸°ì‚¬',
            r'ë”\s*ë³´ê¸°',
            r'ì´ì „\s*ê¸°ì‚¬',
            r'ë‹¤ìŒ\s*ê¸°ì‚¬',
            r'^\s*AD\s*$',
            r'ê´‘ê³ ',
            r'êµ¬ë…',
            r'ë¡œê·¸ì¸',
            r'íšŒì›ê°€ì…'
        ]
        
        for pattern in junk_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                return True
        return False
    
    def _fallback_content_extraction(self, soup):
        """í´ë°± ë³¸ë¬¸ ì¶”ì¶œ"""
        # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        all_divs = soup.find_all(['div', 'article', 'section', 'main'])
        best_content = ""
        
        for div in all_divs:
            text = div.get_text().strip()
            if len(text) > len(best_content) and len(text) > 300:
                # ë„ˆë¬´ ë§ì€ ë§í¬ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
                links = div.find_all('a')
                if len(links) < len(text) / 100:  # í…ìŠ¤íŠ¸ 100ìë‹¹ ë§í¬ 1ê°œ ë¯¸ë§Œ
                    best_content = text
        
        return best_content
    
    def _extract_with_selenium_advanced(self, url):
        """ê³ ê¸‰ Selenium ì¶”ì¶œ"""
        print("ğŸŒ Selenium ê³ ê¸‰ ëª¨ë“œë¡œ ì¬ì‹œë„...")
        
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-web-security')
        options.add_argument('--disable-features=VizDisplayCompositor')
        
        # Google ê´€ë ¨ ì—ëŸ¬ ë°©ì§€
        options.add_argument('--disable-background-networking')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-breakpad')
        options.add_argument('--disable-client-side-phishing-detection')
        options.add_argument('--disable-component-extensions-with-background-pages')
        options.add_argument('--disable-default-apps')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-hang-monitor')
        options.add_argument('--disable-ipc-flooding-protection')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--disable-prompt-on-repost')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-sync')
        options.add_argument('--force-fieldtrials=*BackgroundTracing/default/')
        options.add_argument('--metrics-recording-only')
        options.add_argument('--no-crash-upload')
        options.add_argument('--no-first-run')
        
        options.add_experimental_option('excludeSwitches', ['enable-automation', 'enable-logging'])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option("prefs", {
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_settings.popups": 0,
            "profile.managed_default_content_settings.images": 2
        })
        
        driver = None
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            
            # ì¶”ê°€ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë´‡ íƒì§€ ìš°íšŒ
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("ğŸŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            driver.get(url)
            
            # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 10).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸ )
            time.sleep(3)
            
            # ìŠ¤í¬ë¡¤í•´ì„œ ëª¨ë“  ì½˜í…ì¸  ë¡œë”©
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_with_selectors(soup, self.chosun_selectors['title'])
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_advanced(soup)
            
            if title and content and len(content) > 200:
                print("âœ… Seleniumìœ¼ë¡œ ì„±ê³µì  ì¶”ì¶œ!")
                return {
                    'title': title,
                    'content': content,
                    'url': url,
                    'method': 'Selenium ê³ ê¸‰ ëª¨ë“œ',
                    'confidence': 0.85,
                    'success': True
                }
            else:
                print("âŒ Seleniumìœ¼ë¡œë„ ì¶©ë¶„í•œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                return {
                    'title': title or "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨",
                    'content': content or "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨",
                    'url': url,
                    'method': 'Selenium (ë¶€ë¶„ ì‹¤íŒ¨)',
                    'confidence': 0.3,
                    'success': False,
                    'error': 'Insufficient content extracted'
                }
                
        except Exception as e:
            print(f"âŒ Selenium ì‹¤íŒ¨: {e}")
            return {
                'url': url,
                'method': 'Selenium ì‹¤íŒ¨',
                'success': False,
                'error': str(e)
            }
        finally:
            if driver:
                driver.quit()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    extractor = ImprovedNewsExtractor()
    
    # ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸
    chosun_url = "https://biz.chosun.com/stock/finance/2025/08/19/AMZGKTJ4FFFBZF6VOYDZVJ3V2U/"
    
    print("ğŸš€ ì¡°ì„ ì¼ë³´ ë¹„ì¦ˆ ê³ ê¸‰ ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    
    result = extractor.extract_chosun_biz_advanced(chosun_url)
    
    print("\nğŸ“Š ê²°ê³¼:")
    print("="*80)
    if result.get('success'):
        print(f"âœ… ì„±ê³µ: {result['method']}")
        print(f"ğŸ“° ì œëª©: {result['title']}")
        print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(result['content'])}ì")
        print(f"ğŸ“„ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {result['content'][:200]}...")
        print(f"ğŸ¯ ì‹ ë¢°ë„: {result['confidence']*100}%")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        print(f"ğŸ”§ ì‹œë„í•œ ë°©ë²•: {result.get('method', 'N/A')}")
